\documentclass{article}
\usepackage{fullpage,amssymb,amsmath,amsthm,amsfonts,algorithm,algorithmic,graphicx, subfigure,color}

\def\allow{\mathop{\rm Allow}\nolimits}
\def\qallow{\mathop{{\rm q}{-}{\rm Allow}}\nolimits}
\def\Int{\mathop{\rm Int}\nolimits}
\def\Res{\mathop{\rm Res}\nolimits}
\def\Dis{\mathop{\rm Dis}\nolimits}
\def\graph{\mathop{\rm graph}\nolimits}
\def\codim{\mathop{\rm codim}\nolimits}
\def\eqbd{\mathop{{:}{=}}}
\def\bdeq{\mathop{{=}{:}}}
\def\polylog{\mathop{\rm polylog}\nolimits}

\def\R{\mathbb{R}}
\def\S{\mathcal{S}}
\def\G{\mathcal{G}}
\def\C{\mathbb{C}}
\def\Z{\mathbb{Z}}
\def\F{\mathbb{F}}
\def\H{\mathbb{H}}
\def\k{{\cal A}}
\def\i{{\cal I}}
\def\ee{{\rm e}}
\def\om{{\rm emax}} 
\def\qed{\hfill {$\Box$}}
\def\DD{{\bf\Theta}}
\def\D{{\mathbf\Delta}}
\def\lt{\left}
\def\rt{\right}
\newcommand{\red}[1]{\textcolor{red}{#1}}

\newcommand{\bmat}{\left[ \begin{array}}
\newcommand{\emat}{\end{array} \right]}
\newcommand{\diag}{{\rm diag}}
\newcommand{\sign}{{\rm sign}}
\newcommand{\sep}{{\rm sep}}
\newcommand{\VEC}{{\rm vec}}

\newcommand{\mtf}{\lfloor \frac{m}{2} \rfloor}
\newcommand{\mtc}{\lceil \frac{m}{2} \rceil}
\newcommand{\mt}{\frac{m}{2}}
\newcommand{\dis}{\displaystyle}
\newcommand{\ra}{\rightarrow}

\newcommand{\ignore}[1]{}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{claim}[theorem]{Claim}
\newtheorem{result}[theorem]{Result}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{construction}[theorem]{Construction}
\newtheorem{remark}[theorem]{Remark}

\newcommand{\Sym}{{\operatorname{Sym}}}
\newcommand{\End}{{\operatorname{End}}}
\newcommand{\Fr}[1]{{\|#1\|_{\mathrm{F}}}}
\newcommand{\Err}{{\mathrm{Err}}}
\newcommand{\ve}{\varepsilon}
\newcommand{\smin}{s_{r,n}}
\newcommand{\fmin}{f_{r,n}}

\graphicspath{{figures/}}

\title{A Generalized Randomized Rank-Revealing Factorization}
%Minimizing Communication for Eigenproblems and the Singular Value Decomposition}

\author{Grey Ballard\thanks{CS Division, University of California, Berkeley, CA 94720},
James Demmel\thanks{Mathematics Department and CS Division,
University of California, Berkeley, CA 94720.},
Ioana Dumitriu\thanks{Mathematics Department, University of Washington, Seattle, WA 98195.}, and Chris Melgaard\thanks{CS Division, University of California, Berkeley, CA 94720}}

\begin{document}

\maketitle

\begin{abstract}

\end{abstract}

\section{Introduction} \label{Intro} 

\section{Randomized Rank-Revealing Decompositions} \label{rrdr}

Let $A$ be an $n \times n$ matrix with singular values $\sigma_1 \geq \sigma_2 \geq \ldots \geq \sigma_n$, and assume that there is a ``gap'' in the singular values at level $k$, that is, $\sigma_1/\sigma_k = O(1)$, while $\sigma_k/\sigma_{k+1} \gg 1$. 

Informally speaking, a decomposition of the form $A = URV$ is called \emph{rank revealing} if the following conditions are fulfilled:
\begin{itemize}
\item[1)] $U$ and $V$ are orthogonal/unitary and $R = \left [ \begin{array}{cc} R_{11} & R_{12} \\ O & R_{22} \end{array} \right ]$ is upper triangular, with $R_{11}$  $k \times k$ and  $R_{22}$  $(n-k) \times (n-k)$;
\item[2)] $\sigma_{min}(R_{11})$ is a ``good'' approximation to $\sigma_k$ (at most a factor of a low-degree polynomial in $n$ away from it),
\item[(3)] $\sigma_{max}(R_{22})$ is a ``good'' approximation to $\sigma_{k+1}$ (at most a factor of a low-degree polynomial in $n$ away from it);
\item[(4)] In addition, if $||R_{11}^{-1}R_{12}||_2$ is small (at most a low-degree polynomial in $n$), then the rank-revealing factorization is called  \emph{strong} (as per \cite{GE96}).

\end{itemize}

Rank revealing decompositions are used in rank determination \cite{stewart84}, least square computations \cite{CH92}, 
condition estimation \cite{bischof90a}, etc.,
as well as in divide-and-conquer algorithms for eigenproblems.
For a good survey paper, we recommend \cite{GE96}.

In the paper \cite{DDH07}, we have proposed a \emph{randomized} rank revealing factorization algorithm \textbf{RURV}. Given a matrix $A$, the routine computes a decomposition $A = URV$ with the property that $R$ is a rank-revealing matrix; the way it does it is by ``scrambling'' the columns of $A$ via right multiplication by a uniformly random orthogonal (or unitary) matrix $V^{H}$. The way to obtain such a random matrix (whose described distribution over the manifold of unitary/orthogonal matrices is known as \emph{Haar}) is to start from a matrix $B$ of independent, identically distributed normal variables of mean $0$ and variance $1$, denoted here and throughout the paper by $N(0,1)$. The orthogonal/unitary matrix $V$ obtained from performing the \textbf{QR} algorithm on the matrix $B$ is Haar distributed. 

Performing \textbf{QR} on the resulting matrix $A V^{H}=:\hat{A} = UR$ yields two matrices, $U$ (orthogonal or unitary) and $R$ (upper triangular), and it is immediate to check that $A = URV$. 

\begin{algorithm}
\protect\caption{Function $[U, R, V] =$\textbf{RURV}$(A)$, computes a randomized rank revealing decomposition $A = URV$, with $V$ a Haar matrix.} 
\begin{algorithmic}[1]
\label{rurv}
\STATE Generate a random matrix $B$ with i.i.d. $N(0,1)$ entries. 
\STATE $[V, \hat{R}] = $\textbf{QR}$(B)$.
\STATE $\hat{A} = A \cdot V^{H}$.
\STATE $[U,R] =$ \textbf{QR}$(\hat{A})$.
\STATE Output $R$.
\end{algorithmic}
\end{algorithm}

Similarly to \textbf{RURV} we define the routine \textbf{RULV}, which performs the same kind of computation (and obtains a rank revealing decomposition of $A$), but uses \textbf{QL} instead of \textbf{QR}, and thus obtains a lower triangular matrix in the middle, rather than an upper triangular one. 

Given \textbf{RURV} and \textbf{RULV}, we now can give a method to find a randomized rank-revealing factorization for a product of matrices and inverses of matrices, \emph{without actually computing any of the inverses or matrix products}. This is a very interesting and useful procedure in itself, but we will also use it in Sections \ref{Algs} and \ref{CRA} in the analysis of a single step of our Divide-and-Conquer algorithms. 

Suppose we wish to find a randomized rank-revealing factorization $M_k = URV$ for the matrix $M_k = A_1^{m_1} \cdot A_2^{m_2} \cdot \ldots A_k^{m_k}$, where $A_1, \ldots, A_k$ are given matrices, and $m_1, \ldots, m_k \in \{-1,1\}$, without actually computing $M_k$ or any of the inverses. 

Essentially, the method performs \textbf{RURV} or, depending on the power, \textbf{RULV}, on the last matrix of the product, and then uses a series of \textbf{QR}/\textbf{RQ} to ``propagate'' an orthogonal/unitary matrix to the front of the product, while computing factor matrices from which (if desired) the upper triangular $R$ matrix can be obtained. A similar idea was explored by G.W. Stewart in \cite{Stewart95} to perform graded \textbf{QR}; although it was suggested that such techniques can be also applied to algorithms like $\textbf{URV}$, no randomization was used. 

The algorithm is presented in pseudocode below. 

\begin{algorithm}
\protect\caption{Function $U =$\textbf{GRURV}$(k; A_1, \ldots, A_k; m_1, \ldots, m_k)$, computes the $U$ factor in a randomized rank revealing decomposition of the product matrix $M_k = A_1^{m_1} \cdot A_2^{m_2} \cdot \ldots A_k^{m_k}$, where $m_1, \ldots, m_k \in \{-1,1\}$.} 
\label{grurv}\begin{algorithmic}[1]
%\STATE Generate a random matrix $B$ with i.i.d. $N(0,1)$ entries. 
%\STATE $[V, \hat{R}] = $\textbf{QR}$(B)$.
\IF{$m_k = 1$,}
\STATE $[U,R_k,V] = \textbf{RURV}(A_k)$
\ELSE
\STATE $[U,L_k,V] = \textbf{RULV}(A_k^{H})$
\STATE $R_k =L_k^{H}$ 
\ENDIF
\STATE $U_{current} = U$
\FOR{$i = k-1$ downto $1$}
\IF{$m_i = 1$,}
\STATE $[U,R_i] = \textbf{QR}(A_i \cdot U_{current})$
\STATE $U_{current} = U$
\ELSE 
\STATE $[U,R_i] = \textbf{RQ}(U_{current}^{H} \cdot A_i)$
\STATE $U_{current} = U^H$
\ENDIF
\ENDFOR
\RETURN $U_{current}$, optionally $V, R_1, \ldots, R_k$
\end{algorithmic}
\end{algorithm}

\begin{lemma}
\textbf{GRURV} (\emph{Generalized Randomized URV}) computes the rank-revealing decomposition $M_k = U_{current}R_1^{m_1} \ldots R_k^{m_k} V$. 
\end{lemma}

\begin{proof} Let us examine the case when $k = 2$ ($k>2$ results immediately through simple induction). 

Let us examine the cases:
\begin{enumerate}
\item $m_2 = 1$. In this case, $M_2 = A_1^{m_1} A_2$; the first \textbf{RURV} yields
$M_2 = A_1^{m_1} UR_2V$. \begin{enumerate}
\item if $m_1 = 1$, $M_2 = A_1 UR_2V$; performing \textbf{QR} on $A_1 U$ yields
$M_2= U_{current} R_1 R_2 V$.
%; $R:=R_1 R_2$, as the product of two upper triangular matrices, is also upper triangular. Therefore, de facto, we have obtained a rank-revealing decomposition of $M_2$. Moreover, due to uniqueness, it has to be the same decomposition that we would have obtained, had we performed \textbf{QR} on $M_k V^{H}$. 
\item if $m_1 = -1$, $M_2 = A_1^{-1} UR_2V$; performing \textbf{RQ} on $U^{H}A_1$ yields $M_2 = U_{current} R_1^{-1} R_2 V$.
%; since the inverse of an upper triangular mmatrix is upper triangular, again, $R:= R_1^{-1} R_2$ is upper triangular. Once again we have obtained the same rank-revealing decomposition of $M_2$ as if we had performed \textbf{QR} on $M_2 V^{H}$.
\end{enumerate}
\item $m_2 = -1$. In this case, $M_2 = A_1^{m_1} A_2^{-1}$; the first \textbf{RULV} yields $M_2 = A_1^{m_1} U L_2^{-H} V = A_1^{m_1} U R_2^{-1} V$. \begin{enumerate}
\item if $m_1 = 1$, $M_2 = A_1 U L_2^{-H} V = A_1 U R_2^{-1} V$; performing \textbf{QR} on $A_1 U$ yields $M_2 = U_{current} R_1 R_2^{-1} V$.
%; since $L_2^{-H}$ is an upper triangular matrix, and so $R:= R_1 L_2^{-H}$ is upper triangular, and this is the same rank-revealing decomposition we would have obtained for $M_2$ if we had started with $V$ and performed \textbf{QR} on $M_2 V^{H}$.
\item finally, if $m_2 = -1$,  $M_2 = A_1^{-1} U L_2^{-H} V = A_1^{-1} U R_2^{-1}V$; performing \textbf{RQ} on $U^{H}A_1$ yields $M_2 = U_{current} R_1^{-1} R_2^{-1} V$.
%, and $R:=R_1^{-1} L_2^{-H}$ is upper triangular, again, and the decomposition of $M_2$ is complete. 
\end{enumerate}
\end{enumerate}

Note now that in all cases $M_k = U_{current} R_1^{m_1} \ldots R_k^{m_k} V$. Since the inverse of an upper triangular matrix is upper triangular, and since the product of two upper triangular matrices is upper triangular, it follows that $R:=R_1^{m_1} \ldots R_k^{m_k}$ is upper triangular. Thus, we have obtained a rank-revealing decomposition of $M_k$; the same rank-revealing decomposition as if we have performed $QR$ on $M_k V^{H}$.
\end{proof}

By using the stable \textbf{QR} and \textbf{RQ} algorithms described in \cite{DDH07}, as well as \textbf{RURV}, we can guarantee the following result.

\begin{theorem} \label{thm_grurv} The result of the algorithm \textbf{GRURV} is the same as the result of 
\textbf{RURV} on the (explicitly formed) matrix $M_k$; therefore, given a large gap in the 
singular values of $M_k$, $\sigma_{r+1} \ll \sigma_r \sim \sigma_1$, 
the algorithm \textbf{GRURV} produces a rank-revealing decomposition with high probability.
\end{theorem}

Note that we may also return the matrices $R_1, \ldots, R_k$, from which the factor $R$ can later 
be reassembled, if desired (our algorithms only need $U_{current}$, not $R =R_1^{m_1} \ldots R_k^{m_k}$,
and so we will not compute it).

%Note that it would be easy enough to keep track of the resulting upper triangular matrix $R$ as well, by multiplying the result of \textbf{QR} or the inverse of the matrix obtained through \textbf{RQ} to the left of the current $R$ matrix. However, this would imply taking inverses of (triangular) matrices, which is what we wanted to avoid.
%Armed with \textbf{RURV} and \textbf{RULV}, we now present the generalized eigenvalue decomposition algorithm (following the same notational conventions as in \cite{baidemmelgu94}).
%\subsection{}

\section{Smallest singular value bounds}

The estimates for our main theorem are based on the following result, a more general case of which can be found in \cite{dumitriu?} as Theorem []. 

\begin{definition}
Let $\smin$ be a random variable denoting the smallest singular value of an $r\times r$ corner of an $n\times n$ real Haar matrix.
\end{definition}

\begin{proposition} The probability density function (pdf) of $\smin$, with $r < n/2$, is given by
\[
\fmin(x) = c_{r,n} \frac{1}{\sqrt{x}} (1 - x)^{\frac{1}{2}r(n-r)-1}  {_{2}F_{1}} \left ( \frac{1}{2}(n-r-1), \frac{1}{2}(r-1); \frac{1}{2}(n-1)+1; 1-x \right)~,
\]
where
\[
c_{r,n} = \frac{\frac{1}{2}r(n-r) ~\Gamma \left( \frac{1}{2}(n-r+1) \right) ~\Gamma \left ( \frac{1}{2}(r+1) \right)}{\Gamma \left (\frac{1}{2} \right) ~\Gamma \left ( \frac{1}{2} (n+1) \right)}~.
\]
\end{proposition}

This Proposition allows us to estimate very closely the probability that $\smin$ is small. In particular, the correct scaling for the asymptotics of $\smin$ under $r$ and$\slash$or $n \rightarrow \infty$ was proved to be $\sqrt{r(n-r)}$ (that is, $\smin \sqrt{r(n-r)} = O(1)$ almost surely), which means that the kind of upper bounds one should search for $\smin$ are of the form ``$\smin \leq a/\sqrt{r(n-r)}$'' for some constant $a$. This constant $a$ will depend on how confident we want to be that the inequality holds; if we wish to say that the inequality fails with probability $\delta$, then we will have $a$ as a function of $\delta$. 

\begin{lemma} 
\label{low_bd}
Let $\delta>0$, $r, (n-r)>30$; then $\smin \leq \frac{\delta}{\sqrt{r(n-r)}}$
satisfies $P\left [ \smin \leq \frac{\delta}{\sqrt{r(n-r)}} \right] \leq  2.02 \delta$.
\end{lemma}
\begin{proof} 

What we essentially need to do here is find an upper bound on $\fmin$ which, when integrated over small intervals next to $0$, yields the bound in the Lemma.

We will first upper bound the term $(1-x)^{\frac{1}{2}r(n-r)-1}$ in the expression of $\fmin(x)$ by $1$.

Secondly, we note that the hypergeometric function has all positive arguments, and hence from its definition, it is  monotonically decreasing from $0$ to $1$, and so we bound it by its value at $x=0$. As per \cite[Formula 15.1.20]{Abr_Steg}, 
\[
_{2}F_{1} \left ( \frac{1}{2}(n-r-1), \frac{1}{2}(r-1); \frac{1}{2}(n-1)+1; 1\right) = \frac{\Gamma \left (\frac{1}{2} (n+1) \right) \Gamma\left(\frac{3}{2}\right)}{\Gamma \left ( \frac{1}{2}(n-r+2)\right) \Gamma \left ( \frac{1}{2} (r+2) \right) }~,
\]
and after some obvious cancellation we obtain that 
\[
\fmin(x) \leq \frac{1}{2} r(n-r) \cdot \frac{\Gamma \left ( \frac{1}{2}(n-r+1)\right)}{\Gamma \left (\frac{1}{2}(n-r+2)\right)} \cdot \frac{\Gamma \left(\frac{1}{2}(r+1)\right)}{\Gamma \left ( \frac{1}{2} (r+2)\right)} \cdot \frac{1}{\sqrt{x}}.
\]

The following expansion can be derived from Stirling's formula and is given as a particular case of \cite[Formula 6.1.47]{Abr-Steg} (with $a = 0,~b = 1/2$):
\[
z^{1/2} \frac{\Gamma(z)}{\Gamma(z+ 1/2)} = 1 + \frac{1}{8z} + \frac{1}{128z^2} + o\left(\frac{1}{z^2}\right)~,
\]
as $z$ real and $z \rightarrow \infty$. 
%\red{GB - I reworded the previous sentence because the I was confused by the meaning, please check that I interpreted it correctly.} 
In particular, $z>30$ means that $z^{1/2} \frac{\Gamma(z)}{\Gamma(z+ 1/2)} < 1.01$. %\red{Why give both upper and lower bounds here?  Isn't it always bigger than 1, or is the function not monotonic?  Also, and this is not  important, but if you take $z>30$ I think you can get the square to be less than 1.01 which will save us some space (keeping 2 digits rather than the rounded 3).}

Provided that $r, n-r >30$, we thus have
\[
\fmin(x) \leq 1.01 \cdot \sqrt{ r(n-r)} \sqrt{\frac{r(n-r)}{(r+1)(n-r+1)}} \frac{1}{\sqrt{x}}~,
\]
and so
\[
\fmin(x) \leq 1.01  \cdot \sqrt{ r(n-r)}  \cdot \frac{1}{\sqrt{x}}~.
\]

%Quick integration, plus a simple change of variables, allows us to conclude the following result. 
Note that this last inequality allows us to conclude the following:
\begin{eqnarray*}
P\left [ \smin \leq \frac{\delta}{\sqrt{r(n-r)}} \right] & = & P\left [ \smin^2 \leq \frac{\delta^2}{r(n-r)} \right] \\
& \leq &  1.01 \int_0^{\frac{\delta^2}{r(n-r)}} \sqrt{r(n-r)} \frac{1}{\sqrt{t}} dt  \\
& = &  2.02 \delta. 
\end{eqnarray*}
%The last equality was obtained by the change of variable $t = y/(r(n-r))$. 

\end{proof}

As an immediate corollary to Lemma \ref{low_bd} we obtain the following result, which is what we will actually use in our calculations.

\begin{corollary} 
 \label{lowbd}
 Let $\delta>0$, $r, n-r>30$. Then 
$$P\left [ \frac1\smin \leq \frac{2.02}{\delta}\sqrt{r(n-r)} \right] \geq 1-\delta~.$$
\end{corollary}

%\red{GB - I switched the statement of the corollary to the version we use.}

% Similarly, we can find an upper bound for how big the probability that $\smin$ is larger than $\Delta/\sqrt{r(n-r)}$ is; the proof follows the same constant-upper-bounding steps as the lower bound above, but this time the term $(1-x)^{\frac{1}{2}r(n-r)-1}$ becomes essential.

% Take now some $\Delta>0$; it follows that
% \begin{eqnarray*}
% P\left [ \smin \geq \frac{\Delta}{\sqrt{r(n-r)}} \right] & = & P\left [ \smin^2 \geq \frac{\Delta^2}{r(n-r)} \right] \\
% & \leq &  \frac{1.021}{\sqrt{2}} \int_{\frac{\Delta^2}{r(n-r)}}^1 \sqrt{r(n-r)} \frac{1}{\sqrt{t}} (1-t)^{\frac{1}{2}r(n-r)-1} dt \\
% & \leq & \frac{1.021}{\sqrt{2}} \frac{1}{\Delta} \int_{\Delta^2}^{r(n-r)} \left(1 - \frac{y}{r(n-r)} \right)^{\frac{1}{2}r(n-r)} dy~;
% \end{eqnarray*}
% and by using the fact that $1-x \leq e^{-x}$ for all $x \in (0,1)$ we obtain the following bound.

% \begin{lemma}
% Let $\Delta>1$, $r, (n-r)>20$. Then $\smin \leq \frac{\delta}{\sqrt{r(n-r)}}$
% satisfies $P\left [ \smin \geq \frac{\Delta}{\sqrt{r(n-r)}} \right] \leq \frac{\sqrt{2.042}}{\Delta} e^{-Delta^2/2}$.
% \end{lemma}
% \begin{proof}
% From the inequalities above it follows that
% \begin{eqnarray}
% P\left [ \smin \geq \frac{\Delta}{\sqrt{r(n-r)}} \right] & \leq & \frac{1.021}{\sqrt{2}} \frac{1}{\Delta} \int_{\Delta^2}^{r(n-r)} e^{-y/2} dy \\
% & \leq & \frac{\sqrt{2.042}}{\Delta} e^{-Delta^2/2}~.
% \end{eqnarray}
% \end{proof}

% In particular, the following reverse reading will prove useful, provided that $\delta^2< \frac{2.042}{e}:$
% \begin{eqnarray} \label{upbd}
% P\left [ \smin \geq \frac{\sqrt{2 \ln \frac{1.012 \sqrt{2}}{\delta}}}{\sqrt{r(n-r)}} \right]&  \leq & \delta~.
% \end{eqnarray}


\section{Bounding the probability of failure for RURV}

It was proven in \cite{DDH07} that, with high probability, \textbf{RURV} computes a good rank revealing decomposition of $A$ in the case of $A$ real. Specifically, the quality of the rank-revealing decomposition depends on computing the asymptotics of $\smin$, the smallest singular value of an $r \times r$ submatrix of a Haar-distributed orthogonal $n \times n$ matrix. All the results of \cite{DDH07} can be extended verbatim to Haar-distributed unitary matrices; however, the analysis employed in \cite{DDH07} is not optimal. Using the bounds obtained for $\smin$ in the previous section, we can improve them here. 

We will tighten a bit the argument to obtain one of the upper bounds for $\sigma_{max}(R_{22})$. In addition, the result of \cite{DDH07} states only that \textbf{RURV} is, with high probability, a rank-revealing factorization. Here we strengthen these
results to argue that it is actually a \emph{strong} rank-revealing
factorization (as defined in the Introduction), since with high probability $||R_{11}^{-1} R_{12}||$ will be small. We obtain the following theorem. 


\begin{theorem} \label{thm_rurv}  Let $A$ be an $n \times n$ matrix
  with singular values $\sigma_1, \ldots, \sigma_r, \sigma_{r+1},
  \ldots, \sigma_n$. Let $\delta>0$ be a small number, and let $c>1$, $C >1$ be two constants. 

Let $R$ be the matrix produced by the \textbf{RURV} algorithm on $A$, \emph{in
    exact arithmetic}.  Assume that $r, n-r > 30$. 

Let $\frac{\sigma_1}{\sigma_r} \leq c$, and that $\frac{\sigma_{r}}{\sigma_{r+1}} \geq C n$. Assume further that $\delta C > 1.01$. 

Then, with probability $1-\delta$, the following three events occur:
\begin{eqnarray}
\label{unu} \frac{\delta}{2.02} \frac{\sigma_{r}}{\sqrt{r (n-r)}} & \leq & \sigma_{\min}(R_{11}) \leq \sigma_r ~,\\ 
\label{doi} \sigma_{r+1} & \leq & \sigma_{\max} (R_{22}) \leq \left ( \frac{\left ( \frac{2.02 c}{\delta} \right)^3 \frac{1.01}{\delta C}}{1- \left ( \frac{1.01}{\delta C} \right)^2} (r(n-r))^{3/2} +  \frac{\left ( \frac{2.02 c}{\delta} \right)^2}{1 -\left ( \frac{1.01}{\delta C} \right)^2} r(n-r) + 1 \right ) \sigma_{r+1}~,\\
\label{trei} ||R_{11}^{-1} R_{12}||_2 & \leq & \frac{2.02}{\delta} \frac{1}{1-\left ( \frac{1.01}{\delta C} \right)^{2}} \sqrt{r (n-r)} + \frac{(1.01)^2}{\delta^2 C^2} \frac{1}{1 - \left ( \frac{1.01}{\delta C} \right)^2} ~.
\end{eqnarray}
\end{theorem}

To simplify the expressions, we will make use of the more restrictive assumption $\delta C>\sqrt{2} \cdot 1.01$. This yields the following corollary.

\begin{corollary}
Under all assumptions above and with the additional assumption that $\delta C>\sqrt{2} \cdot 1.01$, the following are true with probability $1-\delta$:
\begin{eqnarray}
\label{unu} \frac{\delta}{2.02} \frac{1}{\sqrt{r (n-r)}} & \leq & \frac{\sigma_{\min}(R_{11})}{\sigma_r} \leq 1 ~,\\ 
\label{doi} 1 & \leq & \frac{\sigma_{\max} (R_{22})}{\sigma_{r+1}} \leq \frac{16.65c^3}{\delta^4C} (r(n-r))^{3/2} +  \frac{8.17c^2}{\delta^2} r(n-r) + 1 ~,\\
\label{trei} ||R_{11}^{-1} R_{12}||_2 & \leq & \frac{4.04}{\delta} \cdot \sqrt{r (n-r)} + 1 ~.
\end{eqnarray}
\end{corollary}
%\red{In hopes of simplifying the presentation of the bounds, I propose we assume that $\delta C> \sqrt 2 \cdot 1.01$.  By doing so, we can bound $1/(1-(1.01/(\delta C))^2)$ by 2 and replace the four appearances of that denominator with a change in constant.  Then maybe we can state every term in the upper bounds as constant times fraction involving powers of $\delta,C,c$ times some power of $r(n-r)$, and the bounds will be easier to parse.}

%\normalsize

\begin{remark} The multiplicative link between $\delta$ and $C$ can easily be seen to mean that the smaller the gap between consecutive singular values $\sigma_r$ and $\sigma_{r+1}$, the greater the uncertainty that the above inequalities hold. Also note that the constant $1.01$ can be made arbitrarily close to $1$, provided that $n$ and $r$ are large enough, as should be clear from the proof of Lemma \ref{low_bd}.
However, one needs $C$ to be strictly bigger than $1$ in order to guarantee that this algorithm will succeed with positive probability, even when $n$ and $r$ are very large indeed. 
%If $C \leq 1$, we cannot provide any guarantees that the algorithm will work.
%\red{Does this mean for $C<1$ that the bounds are sure to fail, or just that we can't prove any guarantee?}
%The $2$ in the condition $\sigma_r/\sigma_{r+1} > 2n$
%  can be replaced by any number strictly greater than $1$.
\end{remark}


\begin{proof} There are two cases of the problem, $r \leq n/2$ and $r> n/2$. 
Let $V$ be the Haar matrix used by the algorithm. 
From \cite[Theorem 2.4-1]{golubvanloan}, 
the singular values of $V(1:r, 1:r)$ when $r>n/2$ consist 
of $(2r-n)$ $1$'s and the singular values of $V((r+1):n ,(r+1):n)$. 
Thus, the case $r>n/2$ reduces to the case $r \leq n/2$.

%The first two relationships, \eqref{unu} and \eqref{doi}, follow from the proof of Theorem 5.2 in \ref{DDH07} and the bound \eqref{lowbd}; specifically, the first follows immediately, while the second requires some explanations, which we provide. The third equation we will prove here. 

The upper bound in inequality \eqref{unu} and the lower bound in inequality \eqref{doi} follow from the Cauchy interlace theorem (see \cite[Theorem 7.3.9]{horn_johnson1}). 
The lower bound in inequality \eqref{unu} follows immediately from \cite[Theorem 5.2]{DDH07} and Corollary \ref{lowbd}.
We provide proofs of the upper bounds of inequalities \eqref{doi} and \eqref{trei}, below. 

Theorem 5.2 from \cite{DDH07} states that 
$$\sigma_{\max}(R_{22}) \leq 3\sigma_{r+1} \cdot \frac{\smin^{-4} \cdot \left ( \frac{\sigma_1}{\sigma_r} \right)^3}{1 - \frac{\sigma_{r+1}^2}{\smin^2 \sigma_r^2}}~;$$
provided that $\sigma_{r+1} < \sigma_r \smin$.
This upper bound is lax, and we tighten it here.
%the factor of $3$ is a lax but convenient upper bound, as 

The proof of Theorem 5.2 actually shows the stronger statement that 
\begin{equation} \label{three_terms}
\sigma_{\max}(R_{22}) \leq \frac{ \sigma_1^3 \frac{\sigma_{r+1}^2}{\smin^{4} \sigma_r^4}}{1 - \frac{\sigma_{r+1}^2}{\smin^2 \sigma_r^2}} + \frac{\sigma_1^2\sigma_{r+1}}{\smin^2\sigma_r^2 - \sigma_{r+1}^2} + \sigma_{r+1}~.
\end{equation}
We note now that, under the assumptions of the problem and by Corollary \ref{lowbd}, with probability at least $1-\delta$, 
\[
\frac{\sigma_{r+1}}{\smin \sigma_r} \leq \frac{1}{Cn} \frac{2.02 \sqrt{r(n-r)}}{\delta}
\] and since $\sqrt{r(n-r)} \leq n/2$, the above simplifies to
\[
\frac{\sigma_{r+1}}{\smin \sigma_r} \leq \frac{1.01}{\delta C}~.
\]
We use now the above together with the fact that, on $[0,1)$, the function $1/(1-x^2)$ is increasing, to obtain that
\[
\frac{1}{1 - \frac{\sigma_{r+1}^2}{\smin^2 \sigma_r^2}} \leq \frac{1}{1 - \left ( \frac{1.01}{\delta C} \right)^2}~.
\]
 At the same time, $\smin^{-1} \leq 2.02 \sqrt{r(n-r)}/\delta$. The first of the three terms on the right hand side of \eqref{three_terms} can be rewritten suitably and bounded from above:
\[
\frac{\left ( \frac{\sigma_1}{\sigma_r} \right)^3 \frac{1}{\smin^3}  \frac{\sigma_{r+1}}{\smin \sigma_r}}{ 1 - \frac{\sigma_{r+1}^2}{\smin^2 \sigma_r^2}} \sigma_{r+1} \leq \frac{c^3 \left ( \frac{2.02 \sqrt{r(n-r)}}{\delta} \right)^3 \frac{1.01}{\delta C}}{1 - \left ( \frac{1.01}{\delta C} \right)^2} \sigma_{r+1}~,
\]
while, after dividing top and bottom by $\smin^2 \sigma_{r}^2$, the second term becomes
\[
\frac{\left ( \frac{\sigma_1}{\sigma_r} \right)^2 \frac{1}{\smin^2}}{ 1 - \frac{\sigma_{r+1}^2}{\smin^2 \sigma_r^2}} \sigma_{r+1} \leq \frac{c^2 \left ( \frac{2.02 \sqrt{r(n-r)}}{\delta} \right)^2}{1 - \left ( \frac{1.01}{\delta C} \right)^2}\sigma_{r+1}~.
\]

 Putting it all together, we obtain the upper bound of \eqref{doi}.  

To prove \eqref{trei}, we use the following notation. Let $A = P \Sigma Q^{H} = P \cdot \diag(\Sigma_1, \Sigma_2) \cdot Q^{H}$ be the singular value decomposition of $A$, where $\Sigma_1 = \diag(\sigma_1, \ldots, \sigma_r)$ and $\Sigma_2 = \diag(\sigma_{r+1}, \ldots, \sigma_n)$. Let $V^H$ be the random unitary matrix in \textbf{RURV}, so that $A=URV$. Then $X = Q^{H}V^{H}$ has the same distribution as $V^{H}$, by virtue of the fact that $V$'s distribution is uniform over unitary matrices. 

Write 
\[
X = \left [ \begin{array}{cc} X_{11} & X_{12} \\ X_{21} & X_{22} \end{array} \right ]~,
\]
where $X_{11}$ is $r \times r$, and $X_{22}$ is $(n-r) \times (n-r)$. 

Then 
\[
U^H P \cdot \Sigma X = R~;
\]
denote $\Sigma \cdot X = [Y_1, Y_2]$ where $Y_1$ is an $n \times r$ matrix and $Y_2$ in an $(n-r) \times n$ one. Since $U^{H}P$ is unitary, it is not hard to check that 
\[
R_{11}^{-1} R_{12} = Y_1^{+} Y_2~,
\]
where $Y_1^{+}$ is the pseudoinverse of $Y_1$, i.e. $Y_1^{+} = (Y_1^H
Y_1)^{-1} Y_1^H$. 

There are two crucial facts that we need  to check here: one is that $R_{11}^{-1}$ actually exists, and the other is that the pseudoinverse (as defined above) is well-defined, that is, that $Y_1$ is full rank. We start with the second one of these facts.

The matrix $Y_1$ is full-rank with probability $1$. This is true due to two facts: the first one is that the first $r$ singular values of $A$, ordered decreasingly on the diagonal of \red{$\Sigma$}, are strictly positive. The second one is that $X$ is Haar distributed, that is, uniformly on the manifold of orthogonal matrices, and hence the probability that its first $r$ columns are dependent is $0$. In particular, this means that, with probability $1$, $X_{11}$ is invertible and $Y_1^{+}$ is well-defined. 

To argue that $R_{11}^{-1}$ exists, note that $Y_1 = P^HU[R_{11};0]$ so rank($Y_1$)$=$rank($R_{11}$) as $P^HU$ is unitary.  Since $Y_1$ is full-rank, it follows that $R_{11}$ is invertible.
%As $Y_1 = \Sigma_1 X$ and $Y_2 = \Sigma_2 X$

Having made sure that the equation relating $R_{11}^{-1} R_{12}$ and $Y_1$ is correct, we proceed to study the right hand side. From the definition of $Y$, we obtain that
\[
Y_1^H Y_1 = X_{11}^H \Sigma_{1}^2 X_{11} + X_{21}^H\Sigma_2^2 X_{21}~~, ~~~~\mbox{and}~~~~ Y_1^H Y_2 = X_{11}^H \Sigma_{1}^2 X_{12} + X_{21}^H\Sigma_2^2 X_{22}~~.
\]
Hence
\[
R_{11}^{-1} R_{12} = \left (X_{11}^{H} \Sigma_1^2 X_{11} + X_{21}^{H} \Sigma_2^2 X_{21} \right)^{-1} \left ( X_{11}^{H} \Sigma_1^2 X_{12} + X_{21}^{H} \Sigma_2^2 X_{22} \right)~.
\]
We split this into two terms. Let $T_1$ be defined as below; we have that
\[
T_1 := \left (X_{11}^{H} \Sigma_1^2 X_{11} + X_{21}^H \Sigma^2_2 X_{21} \right )^{-1} X_{11}^{H} \Sigma_1^2 X_{12} = X_{11}^{-1} \left ( \Sigma_1^2 + (X_{21} X_{11}^{-1})^{H} \Sigma_2^2 (X_{21} X_{11}^{-1}) \right )^{-1} \Sigma_1^2 X_{12}~,
\]
where the last equality reflects the factoring out of $X_{11}^H$ to the left and of $X_{11}$ to the right inside the first parenthesis, followed by cancellation. 
Since $X_{12}$ is a submatrix of a unitary matrix, $||X_{12}|| \leq 1$, and thus
\begin{eqnarray} \label{r-bound}
||T_1|| \leq ||X_{11}^{-1}|| \cdot  || \left (I_r + \Sigma_1^{-2} (X_{21} X_{11}^{-1})^{H} \Sigma_2^2 (X_{21} X_{11}^{-1}) \right )^{-1}|| \leq \frac{1}{\smin} \cdot \frac{1}{1 - \frac{\sigma_{r+1}^2}{ \smin^2 \sigma_r^2}}~,
\end{eqnarray}
where the last inequality follows from the fact that for a matrix $A$ with $||A||<1$, $||(I-A)^{-1}|| \leq \frac{1}{1 - ||A||}$. The right hand side has been obtained by applying norm inequalities and using the fact that $||X_{11}^{-1}|| = \smin^{-1}$. 

The same bounds on $\smin$ and $\sigma_{r+1}/(\smin \sigma_{r})$ apply as before to yield the first summand in \eqref{trei}.

We now apply similar reasoning to the second (remaining) term
\[
T_2:= \left (X_{11}^{H} \Sigma_1^2 X_{11} + X_{21}^H \Sigma^2_2 X_{21} \right )^{-1} X_{21} \Sigma_2^2 X_{22}~;
\]
to yield that 
\begin{eqnarray} \label{r-bound2}
||T_2|| \leq ||X_{11}^{-1}||^2 ||\left ( I_r + \Sigma_1^{-2} (X_{21} X_{11}^{-1})^{H} \Sigma_2^2 (X_{21} X_{11}^{-1})\right)^{-1} || \cdot 
||\Sigma_1^{-2} || \cdot || \Sigma_2^{2}|| \leq \frac{\sigma_{r+1}^2}{\smin^2 \sigma_r^2} \cdot \frac{1}{1 - \frac{\sigma_{r+1}^2}{ \smin^2 \sigma_r^2}}~,
\end{eqnarray}
because $||X_{21}||$ and $||X_{22}|| \leq 1$.

The conditions imposed in the hypothesis give an upper bound which is the second summand in \eqref{trei}. 

From \eqref{r-bound} and \eqref{r-bound2}, the conclusion follows. 
\end{proof}
     
\end{document}
